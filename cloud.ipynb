{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12\n",
    "\n",
    "## Machine Learning in the Cloud\n",
    "\n",
    "![Course Hero](images/hero.png)\n",
    "\n",
    "## Machine Learning Services in the Cloud\n",
    "\n",
    "![AWS ML Stack](images/aws-ml-stack.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need and Access Key Id and Access Key Secret to authenticate with AWS, use yours or ask for it to your instructor.\n",
    "\n",
    "## WARNING: You should NEVER store credentials in a repository. Never save the notebook with the credentials!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_id = \"\"\n",
    "secret_key = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare a session (its not opened until we use it) using that access key and the us-west-2 (Oregon) AWS region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_session = boto3.Session(\n",
    "    aws_access_key_id = access_id,\n",
    "    aws_secret_access_key = secret_key,\n",
    "    region_name = \"us-west-2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekognition\n",
    "\n",
    "![Rekognition](images/rekognition.png)\n",
    "\n",
    "We open a client to the Rekognition service, Artificial Intelligence vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition_client = aws_session.client('rekognition')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the image name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"images/sample.jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_display = Image.open(image_name)\n",
    "display(image_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will identify objects in the image (AWS calls it \"tagging\"). We load the image and send it out to AWS for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(image_name, 'rb') as image:\n",
    "    response = rekognition_client.detect_labels(Image={'Bytes': image.read()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response, depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response[\"Labels\"], depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response[\"Labels\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun begins...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the font and drawing canvas\n",
    "font = ImageFont.truetype(\"Sudo-Bold.ttf\", size=20)\n",
    "draw = ImageDraw.Draw(image_display)\n",
    "w, h = image_display.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one label information\n",
    "name = response[\"Labels\"][0][\"Name\"]\n",
    "print(\"Label Name: \", name)\n",
    "instance = response[\"Labels\"][0][\"Instances\"][0]\n",
    "print(\"Instance: \")\n",
    "pprint(instance)\n",
    "bbox = instance['BoundingBox']\n",
    "\n",
    "# Transform the coordinates from proportional to absolute\n",
    "x0 = int(bbox['Left'] * w)\n",
    "y0 = int(bbox['Top'] * h)\n",
    "x1 = x0 + int(bbox['Width'] * w)\n",
    "y1 = y0 + int(bbox['Height'] * h)\n",
    "print(\"Absolute bbox:\", x0, y0, x1, x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We draw a rectangle and a label over the canvas\n",
    "draw.rectangle(\n",
    "    (x0, y0, x1, y1),\n",
    "    outline=(255, 0, 0),\n",
    "    width=2\n",
    ")\n",
    "draw.text(\n",
    "    (x0, y0),\n",
    "    name,\n",
    "    font=font,\n",
    "    fill=(0, 255, 255)\n",
    ")\n",
    "display(image_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do it for all labels\n",
    "for label in response['Labels']:\n",
    "    name = label['Name']\n",
    "    for instance in label['Instances']:\n",
    "        bbox = instance['BoundingBox']\n",
    "        x0 = int(bbox['Left'] * w)\n",
    "        y0 = int(bbox['Top'] * h)\n",
    "        x1 = x0 + int(bbox['Width'] * w)\n",
    "        y1 = y0 + int(bbox['Height'] * h)\n",
    "        draw.rectangle((x0, y0, x1, y1), outline=(255, 0, 0), width=2)\n",
    "        draw.text((x0, y0), name, font=font, fill=(0, 255, 255))\n",
    "\n",
    "display(image_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polly\n",
    "\n",
    "We can also synthesize audio.\n",
    "\n",
    "![AWS Polly](images/polly.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from contextlib import closing\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from tempfile import gettempdir\n",
    "\n",
    "polly = aws_session.client(\"polly\")\n",
    "\n",
    "try:\n",
    "    # Request speech synthesis\n",
    "    response = polly.synthesize_speech(\n",
    "        Text=\"Machine learning is a great topic\",\n",
    "        OutputFormat=\"mp3\",\n",
    "        VoiceId=\"Joanna\",\n",
    "    )\n",
    "    # We can also try Emma, or in Spanish Mia, Lupe o Lucia\n",
    "    # All the supported languages and voices are documented at https://docs.aws.amazon.com/polly/latest/dg/voicelist.html\n",
    "except (BotoCoreError, ClientError) as error:\n",
    "    # The service returned an error, exit gracefully\n",
    "    print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the audio stream from the response\n",
    "output = \"\"\n",
    "if \"AudioStream\" in response:\n",
    "    # Note: Closing the stream is important because the service throttles on the\n",
    "    # number of parallel connections. Here we are using contextlib.closing to\n",
    "    # ensure the close method of the stream object will be called automatically\n",
    "    # at the end of the with statement's scope.\n",
    "    with closing(response[\"AudioStream\"]) as stream:\n",
    "        output = os.path.join(gettempdir(), \"speech.mp3\")\n",
    "\n",
    "        try:\n",
    "        # Open a file for writing the output as a binary stream\n",
    "            with open(output, \"wb\") as file:\n",
    "                file.write(stream.read())\n",
    "        except IOError as error:\n",
    "            # Could not write to file, exit gracefully\n",
    "            print(error)\n",
    "            sys.exit(-1)\n",
    "\n",
    "else:\n",
    "    # The response didn't contain audio data, exit gracefully\n",
    "    print(\"Could not stream audio\")\n",
    "\n",
    "# Play the audio using the platform's default player\n",
    "if sys.platform == \"win32\":\n",
    "    # The following works on Windows\n",
    "    os.startfile(output)\n",
    "else:\n",
    "    # The following works on macOS and Linux. (Darwin = mac, xdg-open = linux).\n",
    "    opener = \"open\" if sys.platform == \"darwin\" else \"xdg-open\"\n",
    "    subprocess.call([opener, output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe\n",
    "\n",
    "And we can do the opposite, take an audio file an transcribe it into text.\n",
    "\n",
    "![Transcribe Logo](images/transcribe.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "transcribe_client = aws_session.client('transcribe')\n",
    "\n",
    "transcription_job_name=\"Must-be-a-unique-name\"\n",
    "transcript_uri = \"\"\n",
    "\n",
    "transcribe_client.start_transcription_job(\n",
    "    TranscriptionJobName=transcription_job_name,\n",
    "    Media={\"MediaFileUri\": \"s3://david21-publica/sample.mp3\"},\n",
    "    MediaFormat=\"mp3\",\n",
    "    LanguageCode=\"en-US\"\n",
    ")\n",
    "\n",
    "max_tries = 60\n",
    "while max_tries > 0:\n",
    "    max_tries -= 1\n",
    "    job = transcribe_client.get_transcription_job(TranscriptionJobName=transcription_job_name)\n",
    "    job_status = job['TranscriptionJob']['TranscriptionJobStatus']\n",
    "    if job_status in ['COMPLETED', 'FAILED']:\n",
    "        print(f\"Job status is {job_status}.\")\n",
    "        if job_status == 'COMPLETED':\n",
    "            transcript_uri = job['TranscriptionJob']['Transcript']['TranscriptFileUri']\n",
    "            print(\n",
    "                f\"Download the transcript from\\n\"\n",
    "                f\"\\t{transcript_uri}.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Waiting for job to finish. Current status is {job_status}.\")\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(transcript_uri)\n",
    "\n",
    "pprint(response.json(), depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json()[\"results\"][\"transcripts\"][0][\"transcript\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.json()[\"results\"][\"items\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "\n",
    "We can also translate languages.\n",
    "\n",
    "![Translate Logo](images/translate.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = aws_session.client(service_name=\"translate\")\n",
    "\n",
    "result = translate.translate_text(\n",
    "    Text=\"Machine Learning is a very interesting subject\",\n",
    "    SourceLanguageCode=\"en\",\n",
    "    TargetLanguageCode=\"es\"\n",
    ")\n",
    "\n",
    "print('Translated Text: ' + result.get('TranslatedText'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehend\n",
    "\n",
    "We can analyze text to find the meaning of it.\n",
    "\n",
    "![Comprehend](images/comprehend.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehendDetect:\n",
    "    \"\"\"Encapsulates Comprehend detection functions.\"\"\"\n",
    "    def __init__(self, comprehend_client):\n",
    "        \"\"\"\n",
    "        :param comprehend_client: A Boto3 Comprehend client.\n",
    "        \"\"\"\n",
    "        self.comprehend_client = comprehend_client\n",
    "\n",
    "\n",
    "    def detect_languages(self, text):\n",
    "        \"\"\"\n",
    "        Detects languages used in a document.\n",
    "\n",
    "        :param text: The document to inspect.\n",
    "        :return: The list of languages along with their confidence scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.comprehend_client.detect_dominant_language(Text=text)\n",
    "            languages = response['Languages']\n",
    "        except ClientError:\n",
    "            print(\"Couldn't detect languages.\")\n",
    "            raise\n",
    "        else:\n",
    "            return languages\n",
    "\n",
    "    def detect_entities(self, text, language_code):\n",
    "        \"\"\"\n",
    "        Detects entities in a document. Entities can be things like people and places\n",
    "        or other common terms.\n",
    "\n",
    "        :param text: The document to inspect.\n",
    "        :param language_code: The language of the document.\n",
    "        :return: The list of entities along with their confidence scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.comprehend_client.detect_entities(\n",
    "                Text=text, LanguageCode=language_code)\n",
    "            entities = response['Entities']\n",
    "        except ClientError:\n",
    "            print(\"Couldn't detect entities.\")\n",
    "            raise\n",
    "        else:\n",
    "            return entities\n",
    "\n",
    "    def detect_key_phrases(self, text, language_code):\n",
    "        \"\"\"\n",
    "        Detects key phrases in a document. A key phrase is typically a noun and its\n",
    "        modifiers.\n",
    "\n",
    "        :param text: The document to inspect.\n",
    "        :param language_code: The language of the document.\n",
    "        :return: The list of key phrases along with their confidence scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.comprehend_client.detect_key_phrases(\n",
    "                Text=text, LanguageCode=language_code)\n",
    "            phrases = response['KeyPhrases']\n",
    "        except ClientError:\n",
    "            print(\"Couldn't detect phrases.\")\n",
    "            raise\n",
    "        else:\n",
    "            return phrases\n",
    "\n",
    "    def detect_pii(self, text, language_code):\n",
    "        \"\"\"\n",
    "        Detects personally identifiable information (PII) in a document. PII can be\n",
    "        things like names, account numbers, or addresses.\n",
    "\n",
    "        :param text: The document to inspect.\n",
    "        :param language_code: The language of the document.\n",
    "        :return: The list of PII entities along with their confidence scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.comprehend_client.detect_pii_entities(\n",
    "                Text=text, LanguageCode=language_code)\n",
    "            entities = response['Entities']\n",
    "        except ClientError:\n",
    "            print(\"Couldn't detect PII entities.\")\n",
    "            raise\n",
    "        else:\n",
    "            return entities\n",
    "\n",
    "    def detect_sentiment(self, text, language_code):\n",
    "        \"\"\"\n",
    "        Detects the overall sentiment expressed in a document. Sentiment can\n",
    "        be positive, negative, neutral, or a mixture.\n",
    "\n",
    "        :param text: The document to inspect.\n",
    "        :param language_code: The language of the document.\n",
    "        :return: The sentiments along with their confidence scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.comprehend_client.detect_sentiment(\n",
    "                Text=text, LanguageCode=language_code)\n",
    "        except ClientError:\n",
    "            print(\"Couldn't detect sentiment.\")\n",
    "            raise\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "    def detect_syntax(self, text, language_code):\n",
    "        \"\"\"\n",
    "        Detects syntactical elements of a document. Syntax tokens are portions of\n",
    "        text along with their use as parts of speech, such as nouns, verbs, and\n",
    "        interjections.\n",
    "\n",
    "        :param text: The document to inspect.\n",
    "        :param language_code: The language of the document.\n",
    "        :return: The list of syntax tokens along with their confidence scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.comprehend_client.detect_syntax(\n",
    "                Text=text, LanguageCode=language_code)\n",
    "            tokens = response['SyntaxTokens']\n",
    "        except ClientError:\n",
    "            print(\"Couldn't detect syntax.\")\n",
    "            raise\n",
    "        else:\n",
    "            return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = ComprehendDetect(aws_session.client('comprehend'))\n",
    "\n",
    "sample_text = \"Machine learning is an important component of the growing field of data science. Through the use of statistical methods, algorithms are trained to make classifications or predictions, and to uncover key insights in data mining projects. These insights subsequently drive decision making within applications and businesses, ideally impacting key growth metrics. As big data continues to expand and grow, the market demand for data scientists will increase. They will be required to help identify the most relevant business questions and the data to answer them.\"\n",
    "\n",
    "demo_size = 3\n",
    "\n",
    "print(\"Detecting languages.\")\n",
    "languages = comprehend.detect_languages(sample_text)\n",
    "pprint(languages)\n",
    "lang_code = languages[0]['LanguageCode']\n",
    "\n",
    "print(\"Detecting entities.\")\n",
    "entities = comprehend.detect_entities(sample_text, lang_code)\n",
    "print(f\"The first {demo_size} are:\")\n",
    "pprint(entities[:demo_size])\n",
    "\n",
    "print(\"Detecting key phrases.\")\n",
    "phrases = comprehend.detect_key_phrases(sample_text, lang_code)\n",
    "print(f\"The first {demo_size} are:\")\n",
    "pprint(phrases[:demo_size])\n",
    "\n",
    "print(\"Detecting personally identifiable information (PII).\")\n",
    "pii_entities = comprehend.detect_pii(sample_text, lang_code)\n",
    "print(f\"The first {demo_size} are:\")\n",
    "pprint(pii_entities[:demo_size])\n",
    "\n",
    "print(\"Detecting sentiment.\")\n",
    "sentiment = comprehend.detect_sentiment(sample_text, lang_code)\n",
    "print(f\"Sentiment: {sentiment['Sentiment']}\")\n",
    "print(\"SentimentScore:\")\n",
    "pprint(sentiment['SentimentScore'])\n",
    "\n",
    "print(\"Detecting syntax elements.\")\n",
    "syntax_tokens = comprehend.detect_syntax(sample_text, lang_code)\n",
    "print(f\"The first {demo_size} are:\")\n",
    "pprint(syntax_tokens[:demo_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker\n",
    "\n",
    "![Sagemaker Logo](images/sagemaker.jpg)\n",
    "\n",
    "There are also services that grant you access to a full Machine Learning work environment, including data storage, data processing, security, automated workflows, ephemeral compute resources, and services publishing.\n",
    "\n",
    "![MLOps Platform](images/MLOps%20Platform.jpeg)\n",
    "\n",
    "There is also a free service for learning Machine Learning and do simple experiments.\n",
    "\n",
    "[Amazon Sagemaker Studio Lab](https://studiolab.sagemaker.aws)\n",
    "\n",
    "## Hugging Face\n",
    "\n",
    "If you use Sagemaker Studio Lab, or if you want to learn some great Machine Learning tools, read about [Hugging Face](https://huggingface.co).\n",
    "\n",
    "![Hugging Face Logo](images/huggingface-logo.png)\n",
    "\n",
    "## Other Machine Learning cloud providers\n",
    "\n",
    "Of course AWS is not the only Cloud Provider. There are also very complete offers from Google Cloud Platform and Microsoft Azure.\n",
    "\n",
    "And there are also \"small\" services that we can use, almost all of them with a free tier. Some of them are:\n",
    "\n",
    "- [Dataiku](https://www.dataiku.com/product/)\n",
    "- [Google Colab](https://colab.research.google.com)\n",
    "- [JetBrains Datalore](https://datalore.jetbrains.com)\n",
    "- [Kaggle](https://www.kaggle.com)\n",
    "- [Paperspace](https://www.paperspace.com)\n",
    "\n",
    "And of course, [Binder](https://mybinder.org), if only to see and run the notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd4522ee793cbbf1977c4b9c0eaa7d613f5d518c1223b2c56f3f684374957f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
